{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# storage\n",
    "from Storage.TFStorage import TFStorage\n",
    "\n",
    "# Config\n",
    "import Utils.Eva_config_consts as config\n",
    "\n",
    "# Utilities\n",
    "import Utils.folder_utils as folder_utils\n",
    "import Utils.TIMIT_utils as TIMIT_utils\n",
    "import Utils.image_utils as image_utils\n",
    "\n",
    "# Sound utils\n",
    "from Utils.nist_reader import NistReader\n",
    "import Utils.sound_utils as sound_utils\n",
    "\n",
    "# Spectrograms\n",
    "from Utils.SpectrogramFactory import SpectrogramFactory\n",
    "from Utils.Spectrogram import Spectrogram\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sound\n",
    "path_to_train_flac = \"Data/TIMIT/timit/train\"\n",
    "path_to_test_flac = \"Data/TIMIT/timit/test\"\n",
    "\n",
    "#dateset\n",
    "path_to_dataset = \"Data/DataSet/\"\n",
    "\n",
    "#test data\n",
    "path_to_flac_test_folder = \"TestFolder/TestData/fcjf0\"\n",
    "path_to_flac_test_folder_track = \"TestFolder/TestData/fcjf0/SA1.WAV\"\n",
    "\n",
    "#temp\n",
    "temp_folder = \"TestFolder/Temp\"\n",
    "temp_phonemes_folder = \"TestFolder/TempPhonemes\"\n",
    "#temp_image_to_process = \"/Volumes/Storage/processing.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CutIntoChunksAndReshape(spectrum, chunkLength, phoneme, speaker):\n",
    "    totalFeatures = spectrum.shape[1]\n",
    "    #The stepLength is 1 therefore the number of chunks is calculated as follows\n",
    "    numChunks = totalFeatures-chunkLength + 1\n",
    "\n",
    "    #phoneme_subset = None\n",
    "    phoneme_subset = []\n",
    "    Y = []\n",
    "    Z = []\n",
    "\n",
    "    for i in range(numChunks):\n",
    "        chunk = spectrum[:,i:i+chunkLength]\n",
    "        real = np.real(chunk)\n",
    "        imag = np.imag(chunk)\n",
    "        phone_item = np.stack((real,imag), axis=-1)\n",
    "        #adding one more dimetion\n",
    "#         phone_item = phone_item[None, :]\n",
    "#         if(phoneme_subset is None):\n",
    "#             phoneme_subset = phone_item\n",
    "#         else:\n",
    "#             phoneme_subset = np.vstack((phoneme_subset,phone_item))\n",
    "        phoneme_subset.append(phone_item)\n",
    "        Y.append(phoneme)\n",
    "        Z.append(speaker)\n",
    "    return (phoneme_subset, Y, Z)\n",
    "\n",
    "def MergeTwoSubSets(set1, set2):\n",
    "    spectrums = np.vstack((set1[0],set2[0]))\n",
    "    phonemes = set1[1] + set2[1]\n",
    "    speakers = set1[2] + set2[2]\n",
    "    return (spectrums, phonemes, speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paths = folder_utils.reverse_folder(path_to_train_flac, \".WAV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nistReader = NistReader()\n",
    "spectrogramFactory = SpectrogramFactory()\n",
    "phonemeOffset = 64 * 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_set = None\n",
    "\n",
    "#path = pathes[0]\n",
    "for path in paths:\n",
    "    print path\n",
    "    phonemes = TIMIT_utils.parse_phoneme_file(path)\n",
    "    speaker = folder_utils.get_speaker_name(path)\n",
    "\n",
    "    # temp_speaker_folder is used for storing converted to wav audio files.\n",
    "    temp_speaker_folder = os.path.join(temp_folder, speaker)\n",
    "    if not os.path.exists(temp_speaker_folder):\n",
    "        os.makedirs(temp_speaker_folder)\n",
    "\n",
    "    # convert a nist file to a wav file\n",
    "    wav_file = nistReader.Nist2Wav(path, temp_speaker_folder)\n",
    "    for i in range(len(phonemes)):\n",
    "        phoneme = phonemes[i]\n",
    "        #Cutting one phoneme\n",
    "        if i == 0 or i == len(phonemes):\n",
    "            start = int(phoneme[0])\n",
    "            end = int(phoneme[1])\n",
    "        else:\n",
    "            start = int(phoneme[0]) - config.PHONEME_OFFSET\n",
    "            end = int(phoneme[1]) + config.PHONEME_OFFSET\n",
    "\n",
    "        # TODO: create a phoneme object\n",
    "        phone_file = sound_utils.cutPhonemeChunk(wav_file, temp_phonemes_folder, start, end, phoneme[2])\n",
    "        phoneme_spectrogram = spectrogramFactory.create_spectrogram(phone_file) \n",
    "        phone_subset = CutIntoChunksAndReshape(phoneme_spectrogram.spectrogram_values, SPECTROGRAM_CHUNK_LENGTH, phoneme[2], speaker)\n",
    "\n",
    "#         if(combined_set is None):\n",
    "#             combined_set = phone_subset\n",
    "#         else:\n",
    "#             combined_set = MergeTwoSubSets(combined_set, phone_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save(path_to_dataset + \"/\" + \"X\", combined_set[0])\n",
    "np.save(path_to_dataset + \"/\" + \"Phoneme\", combined_set[1])\n",
    "np.save(path_to_dataset + \"/\" + \"Speakers\", combined_set[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.load(path_to_dataset + \"X.npy\")\n",
    "Y = np.load(path_to_dataset + \"Phoneme.npy\")\n",
    "Z = np.load(path_to_dataset + \"Speakers.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phone_subset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "store = pd.HDFStore('TimitStore.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sh = phone_subset[0][0].shape\n",
    "resized = np.resize(phone_subset[0][0],(sh[0]*sh[1]*sh[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'spectrums':resized, 'phoneme':phone_subset[1][0], 'spectrum':phone_subset[2][0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "store.put('d1', df, format='table', data_columns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### TF Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to(data_set, name):\n",
    "  \"\"\"Converts a dataset to tfrecords.\"\"\"\n",
    "  images = data_set.images\n",
    "  labels = data_set.labels\n",
    "  num_examples = data_set.num_examples\n",
    "\n",
    "  if images.shape[0] != num_examples:\n",
    "    raise ValueError('Images size %d does not match label size %d.' %\n",
    "                     (images.shape[0], num_examples))\n",
    "  rows = images.shape[1]\n",
    "  cols = images.shape[2]\n",
    "  depth = images.shape[3]\n",
    "\n",
    "  filename = os.path.join(FLAGS.directory, name + '.tfrecords')\n",
    "  print('Writing', filename)\n",
    "  writer = tf.python_io.TFRecordWriter(filename)\n",
    "  for index in range(num_examples):\n",
    "    image_raw = images[index].tostring()\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'height': _int64_feature(rows),\n",
    "        'width': _int64_feature(cols),\n",
    "        'depth': _int64_feature(depth),\n",
    "        'label': _int64_feature(int(labels[index])),\n",
    "        'image_raw': _bytes_feature(image_raw)}))\n",
    "    writer.write(example.SerializeToString())\n",
    "  writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spectrum = phone_subset[0][0]\n",
    "phoneme = phone_subset[1][0]\n",
    "speaker = phone_subset[2][0]\n",
    "\n",
    "spectrum_size = phone_subset[0][0].shape\n",
    "rows = spectrum_size[0]\n",
    "cols = spectrum_size[1]\n",
    "depth = spectrum_size[2]\n",
    "\n",
    "SPECTRUM_VALUES_SIZE = rows * cols * depth\n",
    "\n",
    "spectrum_raw = phone_subset[0][0].tostring()\n",
    "example = tf.train.Example(features=tf.train.Features(feature={\n",
    "    'height': _int64_feature(rows),\n",
    "    'width': _int64_feature(cols),\n",
    "    'depth': _int64_feature(depth),\n",
    "    'phoneme': _bytes_feature(phoneme),\n",
    "    'speaker': _bytes_feature(speaker),\n",
    "    'image_raw': _bytes_feature(spectrum_raw)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = os.path.join('TimitStore' + '.tfrecords')\n",
    "print('Writing', filename)\n",
    "writer = tf.python_io.TFRecordWriter(filename)\n",
    "for i in range(20):\n",
    "    writer.write(example.SerializeToString())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_and_decode(filename_queue):\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    \n",
    "    features = tf.parse_single_example(\n",
    "      serialized_example,\n",
    "      # Defaults are not specified since both keys are required.\n",
    "      features={\n",
    "        'phoneme': tf.FixedLenFeature([], tf.string),\n",
    "        'speaker': tf.FixedLenFeature([], tf.string),\n",
    "        'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "      })\n",
    "\n",
    "    # Convert from a scalar string tensor (whose single string has\n",
    "    # length mnist.IMAGE_PIXELS) to a uint8 tensor with shape\n",
    "    # [mnist.IMAGE_PIXELS].\n",
    "    spectrum = tf.decode_raw(features['image_raw'], tf.float64)\n",
    "    spectrum.set_shape([SPECTRUM_VALUES_SIZE])\n",
    "\n",
    "    # OPTIONAL: Could reshape into a 28x28 image and apply distortions\n",
    "    # here.  Since we are not applying any distortions in this\n",
    "    # example, and the next step expects the image to be flattened\n",
    "    # into a vector, we don't bother.\n",
    "\n",
    "    # Convert from [0, 255] -> [-0.5, 0.5] floats.\n",
    "    #image = tf.cast(image, tf.float32) * (1. / 255) - 0.5\n",
    "\n",
    "    # Convert label from a scalar uint8 tensor to an int32 scalar.\n",
    "    phoneme = tf.cast(features['phoneme'], tf.string)\n",
    "    speaker = tf.cast(features['speaker'], tf.string)\n",
    "\n",
    "    return spectrum, phoneme, speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    filename_queue = tf.train.string_input_producer([filename])\n",
    "    batch_size = 10\n",
    "    spectrum, phoneme, speaker = read_and_decode(filename_queue)\n",
    "    # Shuffle the examples and collect them into batch_size batches.\n",
    "    # (Internally uses a RandomShuffleQueue.)\n",
    "    # We run this in two threads to avoid being a bottleneck.\n",
    "    spectrums, phonemes = tf.train.shuffle_batch(\n",
    "        [spectrum, phoneme], batch_size=batch_size, num_threads=2,\n",
    "        capacity=10 + 3 * batch_size,\n",
    "        # Ensures a minimum amount of shuffling of examples.\n",
    "        min_after_dequeue=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer([filename])\n",
    "spectrum, phoneme, speaker = read_and_decode(filename_queue)\n",
    "\n",
    "# Required. See below for explanation\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "spectrum_val_1, phoneme_val_1, speaker_val_1 = sess.run([spectrum, phoneme, speaker])\n",
    "spectrum_val_2, phoneme_val_2, speaker_val_2 = sess.run([spectrum, phoneme, speaker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
